---
- name: Setting fresh_deploy fact...
  ansible.builtin.set_fact:
    fresh_deploy: true
  when: proxmox_vm_exists.proxmox_vms == [] or deploy_mode == "redeploy"

- name: Checking for the correct deploy mode...
  ansible.builtin.fail:
    msg: |
      "{{ custom_vm_name | default(vm_name) }} Virtual Machine doesn't exist, use the deploy command first!"
  when:
    - fresh_deploy is ansible.builtin.truthy
    - role_only is ansible.builtin.truthy
      or role_only_wp is ansible.builtin.truthy
      or single_role is defined

- name: Cloning and configuring VM...
  delegate_to: localhost
  become: false
  when:
    - not role_only
    - not role_only_wp
    - single_role is not defined
  block:
    - name: Looking up template and cloning the VM...
      when: fresh_deploy
      block:
        - name: Looking up {{ vm_template }} info...
          community.proxmox.proxmox_vm_info:
            name: "{{ vm_template }}"
          register: proxmox_vm_template
          until: proxmox_vm_template is not failed
          retries: 24
          delay: 5

        - name: MISSING TEMPLATE
          ansible.builtin.fail:
            msg: |
              Error following template not found:
                vm_template: {{ vm_template }}
          when: proxmox_vm_template.proxmox_vms == []

        # Excluding failed hosts to get a list of available hosts still in play
        - name: Getting available play hosts...
          ansible.builtin.set_fact:
            available_play_hosts: "{{ ansible_play_hosts_all | difference(ansible_failed_hosts | default([])) | sort }}"

        # Since Proxmox doesn't support locking or queuing operations, creating a limiter file
        # to make sure only one host in the batch is cloning at a time
        # After cloning is done the hosts will go back to the default parallelism
        - name: Waiting until the previous VM is the batch cloned...
          when: available_play_hosts.index(inventory_hostname) != 0
          delegate_to: localhost
          become: false
          ansible.builtin.wait_for:
            path: /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{
              available_play_hosts[available_play_hosts.index(inventory_hostname) - 1] }}.done
            # Timeout is based on the number of hosts in the batch
            # Increasing the timeout by the operation timeout for each host in the batch
            timeout: "{{ proxmox_machine_operations_operation_timeout * available_play_hosts.index(inventory_hostname) }}"

        # Cloning needs to happen on the same node where the template is located
        # After that the target node is used to move the VM to the correct node
        - name: Cloning {{ custom_vm_name | default(vm_name) }} VM...
          block:
            - name: Getting the time...
              ansible.builtin.setup:
                filter: "*_time"

            # Node is where the template is located, will not work if the template is not on the same node
            # The next task will move the VM to the correct node before starting it
            - name: Cloning {{ custom_vm_name | default(vm_name) }} VM...
              community.proxmox.proxmox_kvm:
                clone: "{{ vm_template }}"
                description: "{{ vm_description }}"
                full: "{{ false if linked_clone_status else true }}"
                name: "{{ custom_vm_name | default(vm_name) }}"
                newid: "{{ proxmox_vm_exists.proxmox_vms[0].vmid | default(machine_operations_proxmox_vmid) }}"
                node: "{{ proxmox_vm_template.proxmox_vms[0].node }}"
                storage: "{{ proxmox_storage if not linked_clone_status else omit }}"
                target: "{{ proxmox_node }}"
                timeout: "{{ proxmox_machine_operations_operation_timeout }}"

            - name: Creating /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{ inventory_hostname }}.done file...
              delegate_to: localhost
              become: false
              ansible.builtin.file:
                path: /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{ inventory_hostname }}.done
                state: touch
                mode: "0644"

          # Using rescue to remove disks left over from failed cloning attempts
          rescue:
            # When cloning fails first making sure the limiter file is created
            # so the next host in the batch can proceed with cloning
            - name: Creating /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{ inventory_hostname }}.done file...
              delegate_to: localhost
              become: false
              ansible.builtin.file:
                path: /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{ inventory_hostname }}.done
                state: touch
                mode: "0644"

            - name: Removing {{ custom_vm_name | default(vm_name) }} VM...
              community.proxmox.proxmox_kvm:
                name: "{{ custom_vm_name | default(vm_name) }}"
                vmid: "{{ proxmox_vm_exists.proxmox_vms[0].vmid | default(machine_operations_proxmox_vmid) }}"
                state: absent
                force: true
                timeout: "{{ proxmox_machine_operations_operation_timeout }}"

            - name: Getting the time...
              ansible.builtin.setup:
                filter: "*_time"
              register: proxmox_disk_cleanup_start_time

            - name: Including disk_cleanup tasks...
              ansible.builtin.include_tasks: disk_cleanup.yml

            - name: Removing {{ custom_vm_name | default(vm_name) }} VM cloning limiter file...
              delegate_to: localhost
              become: false
              ansible.builtin.file:
                path: /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{ inventory_hostname }}.done
                state: absent

            - name: FAILED CLONING ATTEMPT
              ansible.builtin.fail:
                msg: |
                  Cloning {{ custom_vm_name | default(vm_name) }} failed.
                  Check the errors above for this host and try again.

    # Writing the file here in case the non-existing machines are in the same batch as existing ones
    # Otherwise the non-existing will just timeout waiting for the previous host to finish cloning
    - name: Creating /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{ inventory_hostname }}.done file...
      delegate_to: localhost
      become: false
      ansible.builtin.file:
        path: /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{ inventory_hostname }}.done
        state: touch
        mode: "0644"
      when: not fresh_deploy

    - name: Migrating {{ custom_vm_name | default(vm_name) }} VM to {{ proxmox_node }} node...
      community.proxmox.proxmox_kvm:
        name: "{{ custom_vm_name | default(vm_name) }}"
        node: "{{ proxmox_node }}"
        migrate: true
      retries: 12
      delay: 5

    - name: Resizing {{ custom_vm_name | default(vm_name) }} VM disk...
      community.proxmox.proxmox_disk:
        name: "{{ custom_vm_name | default(vm_name) }}"
        vmid: "{{ proxmox_vm_exists.proxmox_vms[0].vmid | default(machine_operations_proxmox_vmid) }}"
        disk: "{{ machine_operations_proxmox_os_disk }}"
        size: "{{ hardware_primary_disk_size }}G"
        state: resized
      # hardware_primary_disk_size variable usually comes from Providentia
      when: hardware_primary_disk_size is defined

    - name: Updating {{ custom_vm_name | default(vm_name) }} VM configuration...
      community.proxmox.proxmox_kvm:
        autostart: true
        cores: "{{ omit if not hardware_cpu else hardware_cpu }}"
        hotplug: "{{ machine_operations_proxmox_hotplug | join(',') }}"
        memory: "{{ omit if not hardware_ram else (hardware_ram * 1024) }}"
        name: "{{ custom_vm_name | default(vm_name) }}"
        vmid: "{{ proxmox_vm_exists.proxmox_vms[0].vmid | default(machine_operations_proxmox_vmid) }}"
        node: "{{ proxmox_node }}"
        onboot: "{{ machine_operations_proxmox_vm_start_on_boot }}"
        timeout: "{{ proxmox_machine_operations_operation_timeout }}"
        update: true

    - name: Attaching following networks interfaces...
      community.proxmox.proxmox_nic:
        name: "{{ custom_vm_name | default(vm_name) }}"
        vmid: "{{ proxmox_vm_exists.proxmox_vms[0].vmid | default(machine_operations_proxmox_vmid) }}"
        interface: "net{{ idx }}"
        bridge: "{{ nic.cloud_id }}"
        mtu: "{{ nic.mtu | default(1) }}"
        firewall: "{{ machine_operations_proxmox_enable_firewall }}"
      loop_control:
        loop_var: nic
        index_var: idx
        label: "{{ nic.cloud_id }}"
      loop: "{{ interfaces }}"

    - name: Starting {{ custom_vm_name | default(vm_name) }} VM... # noqa: no-handler
      community.proxmox.proxmox_kvm:
        name: "{{ custom_vm_name | default(vm_name) }}"
        vmid: "{{ proxmox_vm_exists.proxmox_vms[0].vmid | default(machine_operations_proxmox_vmid) }}"
        state: started
        timeout: "{{ proxmox_machine_operations_operation_timeout }}"
      retries: 6
      delay: 5

    - name: Removing cloning limiter file...
      delegate_to: localhost
      become: false
      ansible.builtin.file:
        path: /tmp/proxmox_vm_clone_{{ project_fullname | default('') }}_{{ inventory_hostname }}.done
        state: absent
